#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests
import json
import re
from datetime import datetime
import time
import os
import concurrent.futures
from urllib.parse import urlparse

class IPTVUpdater:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.all_channels = []
        self.log_messages = []
        self.repository_owner = os.environ.get('GITHUB_REPOSITORY_OWNER', 'mymsnn')
        self.repository_name = os.environ.get('GITHUB_REPOSITORY', 'DailyIPTV').split('/')[-1]
        
    def log(self, message):
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        log_message = f"[{timestamp}] {message}"
        print(log_message)
        self.log_messages.append(log_message)
        
    def load_sources(self):
        """åŠ è½½æºåˆ—è¡¨"""
        try:
            with open('scripts/sources_list.json', 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            self.log(f"åŠ è½½æºåˆ—è¡¨å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤çš„æºåˆ—è¡¨
            return {
                "sources": [
                    "https://raw.githubusercontent.com/iptv-org/iptv/master/index.m3u",
                    "https://raw.githubusercontent.com/fanmingming/live/main/tv/m3u/global.m3u"
                ],
                "backup_sources": [
                    "https://gitlab.com/iptv-org/iptv/-/raw/master/index.m3u",
                    "https://raw.githubusercontent.com/EvilCult/iptv-m3u-maker/master/m3u/index.m3u"
                ]
            }
    
    def fetch_source(self, url, timeout=15):
        """è·å–å•ä¸ªæº"""
        try:
            self.log(f"æ­£åœ¨è·å–: {url}")
            response = self.session.get(url, timeout=timeout)
            response.encoding = 'utf-8'
            if response.status_code == 200:
                return response.text
            else:
                self.log(f"è·å–å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return None
        except Exception as e:
            self.log(f"è·å–å¼‚å¸¸: {e}")
            return None
    
    def parse_m3u(self, content, source_url):
        """è§£æM3Uå†…å®¹"""
        channels = []
        current_channel = {}
        lines = content.splitlines()
        
        for i, line in enumerate(lines):
            line = line.strip()
            if not line:
                continue
                
            if line.startswith('#EXTINF'):
                # è§£æé¢‘é“ä¿¡æ¯
                current_channel = {'raw_extinf': line}
                # æå–é¢‘é“åç§°
                name_match = re.search(r',(?P<name>.*)$', line)
                if name_match:
                    current_channel['name'] = name_match.group('name').strip()
                else:
                    current_channel['name'] = f"Unknown_{i}"
                    
            elif line.startswith('http://') or line.startswith('https://') or line.startswith('rtsp://') or line.startswith('rtmp://'):
                if current_channel:
                    current_channel['url'] = line
                    current_channel['source'] = source_url
                    channels.append(current_channel)
                    current_channel = {}
        
        self.log(f"ä»è¯¥æºè§£æå‡º {len(channels)} ä¸ªé¢‘é“")
        return channels
    
    def is_url_accessible(self, channel, timeout=5):
        """æ£€æŸ¥URLæ˜¯å¦å¯è®¿é—® - æ”¹è¿›ç‰ˆæœ¬"""
        url = channel['url']
        try:
            # å…ˆæ£€æŸ¥URLæ ¼å¼æ˜¯å¦æœ‰æ•ˆ
            parsed_url = urlparse(url)
            if not parsed_url.scheme or not parsed_url.netloc:
                return False, "æ— æ•ˆçš„URLæ ¼å¼"
            
            # å¯¹äºæŸäº›ç±»å‹çš„URLï¼Œä½¿ç”¨æ›´å¿«çš„æ£€æŸ¥æ–¹å¼
            if 'youtube.com' in url or 'youtu.be' in url:
                return True, "YouTubeé“¾æ¥ï¼ˆè·³è¿‡éªŒè¯ï¼‰"
            
            if 'twitch.tv' in url:
                return True, "Twitché“¾æ¥ï¼ˆè·³è¿‡éªŒè¯ï¼‰"
            
            # å‘é€HEADè¯·æ±‚æ£€æŸ¥
            response = requests.head(url, timeout=timeout, allow_redirects=True, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })
            
            if response.status_code in [200, 302, 301]:
                return True, f"çŠ¶æ€ç : {response.status_code}"
            else:
                return False, f"çŠ¶æ€ç : {response.status_code}"
                
        except requests.exceptions.Timeout:
            return False, "è¿æ¥è¶…æ—¶"
        except requests.exceptions.ConnectionError:
            return False, "è¿æ¥é”™è¯¯"
        except requests.exceptions.RequestException as e:
            return False, f"è¯·æ±‚å¼‚å¸¸: {str(e)}"
        except Exception as e:
            return False, f"æœªçŸ¥é”™è¯¯: {str(e)}"
    
    def validate_channels_parallel(self, channels, max_workers=10):
        """å¹¶è¡ŒéªŒè¯é¢‘é“"""
        valid_channels = []
        validation_results = []
        
        self.log(f"å¼€å§‹å¹¶è¡ŒéªŒè¯ {len(channels)} ä¸ªé¢‘é“...")
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰éªŒè¯ä»»åŠ¡
            future_to_channel = {
                executor.submit(self.is_url_accessible, channel): channel 
                for channel in channels
            }
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for i, future in enumerate(concurrent.futures.as_completed(future_to_channel)):
                channel = future_to_channel[future]
                try:
                    is_valid, message = future.result()
                    if is_valid:
                        valid_channels.append(channel)
                    validation_results.append({
                        'channel': channel['name'],
                        'url': channel['url'],
                        'valid': is_valid,
                        'message': message
                    })
                    
                    if (i + 1) % 50 == 0:
                        self.log(f"å·²éªŒè¯ {i + 1}/{len(channels)} ä¸ªé¢‘é“")
                        
                except Exception as e:
                    validation_results.append({
                        'channel': channel['name'],
                        'url': channel['url'],
                        'valid': False,
                        'message': f"éªŒè¯å¼‚å¸¸: {str(e)}"
                    })
        
        # ä¿å­˜è¯¦ç»†çš„éªŒè¯ç»“æœ
        with open('logs/validation_details.json', 'w', encoding='utf-8') as f:
            json.dump(validation_results, f, ensure_ascii=False, indent=2)
        
        return valid_channels, validation_results
    
    def filter_quality_channels(self, channels):
        """è¿‡æ»¤é«˜è´¨é‡é¢‘é“"""
        quality_channels = []
        
        for channel in channels:
            url = channel['url']
            name = channel['name'].lower()
            
            # è¿‡æ»¤æ‰æ˜æ˜¾çš„ä½è´¨é‡æº
            if any(bad_keyword in url for bad_keyword in ['/hls/', '/live/', 'm3u8?']):
                # è¿™äº›å¯èƒ½æ˜¯éœ€è¦ç‰¹æ®Šå¤„ç†çš„æµï¼Œæš‚æ—¶ä¿ç•™
                pass
            
            # è¿‡æ»¤æ‰åç§°ä¸­åŒ…å«æ˜æ˜¾æ— æ•ˆä¿¡æ¯çš„é¢‘é“
            if any(bad_name in name for bad_name in ['test', 'example', 'demo', 'æ— æ•ˆ', 'æµ‹è¯•']):
                continue
                
            # ä¿ç•™çŸ¥åç”µè§†å°å’Œé«˜è´¨é‡æº
            if any(good_keyword in name for good_keyword in [
                'cctv', 'å¤®è§†', 'å«è§†', 'æ¹–å—', 'æµ™æ±Ÿ', 'æ±Ÿè‹', 'åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿ä¸œ',
                'bbc', 'cnn', 'disney', 'discovery', 'national geographic'
            ]):
                quality_channels.append(channel)
                continue
                
            # å…¶ä»–é¢‘é“ä¹Ÿä¿ç•™ï¼Œä½†æ ‡è®°ä¸ºéœ€è¦éªŒè¯
            quality_channels.append(channel)
        
        return quality_channels
    
    def categorize_channel(self, channel_name):
        """åˆ†ç±»é¢‘é“"""
        name_lower = channel_name.lower()
        
        # å¤®è§†åˆ†ç±»
        cctv_keywords = ['cctv', 'å¤®è§†', 'ä¸­å¤®']
        if any(keyword in name_lower for keyword in cctv_keywords):
            return 'cctv'
        
        # å«è§†åˆ†ç±»
        satellite_keywords = ['å«è§†', 'tvb', 'å‡¤å‡°', 'æ˜Ÿç©º', 'æ¹–å—', 'æµ™æ±Ÿ', 'æ±Ÿè‹', 'ä¸œæ–¹', 'åŒ—äº¬']
        if any(keyword in name_lower for keyword in satellite_keywords):
            return 'satellite'
        
        # åœ°æ–¹å°åˆ†ç±»
        local_keywords = ['éƒ½å¸‚', 'æ–°é—»', 'æ°‘ç”Ÿ', 'å…¬å…±', 'æ•™è‚²', 'å°‘å„¿', 'ä½“è‚²', 'å½±è§†', 'ç»¼è‰º']
        if any(keyword in name_lower for keyword in local_keywords):
            return 'local'
        
        # å›½é™…é¢‘é“
        international_keywords = ['bbc', 'cnn', 'nhk', 'fox', 'hbo', 'disney', 'discovery', 'å›½å®¶åœ°ç†']
        if any(keyword in name_lower for keyword in international_keywords):
            return 'international'
        
        return 'other'
    
    def generate_m3u_content(self, channels, title="ç›´æ’­æº"):
        """ç”ŸæˆM3Uå†…å®¹"""
        header = f"""#EXTM3U
#EXTENC: UTF-8
# Generated: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')}
# Updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
# Title: {title}
# Total Channels: {len(channels)}
# For personal testing and research purposes only.

"""
        content = header
        for channel in channels:
            content += f"{channel['raw_extinf']}\n{channel['url']}\n"
        
        return content
    
    def update_readme(self, stats):
        """æ›´æ–°READMEæ–‡ä»¶"""
        try:
            with open('README.md', 'r', encoding='utf-8') as f:
                readme_content = f.read()
            
            base_url = f"https://raw.githubusercontent.com/{self.repository_owner}/{self.repository_name}/main/outputs"
            update_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            
            live_sources_section = f"""
## ğŸ“¡ ç›´æ’­æºåœ°å€

ä»¥ä¸‹æ˜¯æœ€æ–°çš„ç›´æ’­æºåœ°å€ï¼ˆæœ€åæ›´æ–°: {update_time}ï¼‰ï¼š

### ğŸŒ å®Œæ•´åˆ—è¡¨ï¼ˆå·²éªŒè¯ï¼‰
- **M3Uæ ¼å¼**: [{base_url}/full_validated.m3u]({base_url}/full_validated.m3u)
- æœ‰æ•ˆé¢‘é“: {stats['valid_channels']} ä¸ª
- æœ‰æ•ˆæ€§æ¯”ä¾‹: {stats['validity_ratio']:.2%}

### ğŸŒ åŸå§‹åˆ—è¡¨ï¼ˆæœªéªŒè¯ï¼‰
- **M3Uæ ¼å¼**: [{base_url}/full_raw.m3u]({base_url}/full_raw.m3u)
- æ€»é¢‘é“: {stats['total_channels']} ä¸ª

### ğŸ“º åˆ†ç±»åˆ—è¡¨
- **å¤®è§†é¢‘é“**: [{base_url}/cctv.m3u]({base_url}/cctv.m3u) ({stats['categories']['cctv']} ä¸ªé¢‘é“)
- **å«è§†é¢‘é“**: [{base_url}/satellite.m3u]({base_url}/satellite.m3u) ({stats['categories']['satellite']} ä¸ªé¢‘é“)
- **åœ°æ–¹å°**: [{base_url}/local.m3u]({base_url}/local.m3u) ({stats['categories']['local']} ä¸ªé¢‘é“)
- **å›½é™…é¢‘é“**: [{base_url}/international.m3u]({base_url}/international.m3u) ({stats['categories']['international']} ä¸ªé¢‘é“)

### ğŸ“Š ç»Ÿè®¡ä¿¡æ¯
- **æ›´æ–°è€—æ—¶**: {stats['duration_seconds']} ç§’
- **éªŒè¯è€—æ—¶**: {stats['validation_seconds']} ç§’
- **æˆåŠŸæºæ•°**: {stats['sources_successful']} ä¸ª
- **æ›´æ–°æ—¶é—´**: {stats['update_time']}

### ğŸš€ æ¨èä½¿ç”¨
å»ºè®®ä½¿ç”¨ **å·²éªŒè¯åˆ—è¡¨** è·å¾—æ›´å¥½çš„è§‚çœ‹ä½“éªŒï¼

---

"""
            
            if '## ğŸ“¡ ç›´æ’­æºåœ°å€' in readme_content:
                pattern = r'## ğŸ“¡ ç›´æ’­æºåœ°å€.*?---'
                updated_readme = re.sub(pattern, live_sources_section.strip(), readme_content, flags=re.DOTALL)
            else:
                updated_readme = readme_content.replace('# DailyIPTV ğŸ“º', f'# DailyIPTV ğŸ“º\n{live_sources_section}')
            
            with open('README.md', 'w', encoding='utf-8') as f:
                f.write(updated_readme)
            
            self.log("READMEæ–‡ä»¶æ›´æ–°æˆåŠŸï¼")
            return True
            
        except Exception as e:
            self.log(f"æ›´æ–°READMEæ–‡ä»¶å¤±è´¥: {e}")
            return False

    def run(self):
        """ä¸»è¿è¡Œå‡½æ•°"""
        start_time = time.time()
        self.log("=== å¼€å§‹æ›´æ–°IPTVç›´æ’­æº ===")
        
        # åŠ è½½é…ç½®
        self.sources_config = self.load_sources()
        
        # è·å–æ‰€æœ‰æº
        all_channels = []
        successful_sources = 0
        
        for source_url in self.sources_config['sources']:
            content = self.fetch_source(source_url)
            if content:
                channels = self.parse_m3u(content, source_url)
                all_channels.extend(channels)
                successful_sources += 1
                time.sleep(1)
        
        # å¦‚æœä¸»è¦æºå¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æº
        if successful_sources == 0 and self.sources_config['backup_sources']:
            self.log("ä¸»è¦æºå…¨éƒ¨å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æº...")
            for backup_url in self.sources_config['backup_sources']:
                content = self.fetch_source(backup_url)
                if content:
                    channels = self.parse_m3u(content, backup_url)
                    all_channels.extend(channels)
                    successful_sources += 1
                    time.sleep(1)
        
        # å»é‡ï¼šåŸºäºURLå»é‡
        unique_channels = {}
        for channel in all_channels:
            url = channel['url']
            if url not in unique_channels:
                unique_channels[url] = channel
        
        unique_channels_list = list(unique_channels.values())
        self.log(f"å»é‡åé¢‘é“æ•°é‡: {len(unique_channels_list)}")
        
        # ä¿å­˜åŸå§‹åˆ—è¡¨ï¼ˆæœªéªŒè¯ï¼‰
        raw_content = self.generate_m3u_content(unique_channels_list, "åŸå§‹ç›´æ’­æºï¼ˆæœªéªŒè¯ï¼‰")
        with open('outputs/full_raw.m3u', 'w', encoding='utf-8') as f:
            f.write(raw_content)
        
        # è¿‡æ»¤é«˜è´¨é‡é¢‘é“
        quality_channels = self.filter_quality_channels(unique_channels_list)
        self.log(f"è´¨é‡è¿‡æ»¤åé¢‘é“æ•°é‡: {len(quality_channels)}")
        
        # å¹¶è¡ŒéªŒè¯é¢‘é“
        validation_start = time.time()
        valid_channels, validation_results = self.validate_channels_parallel(quality_channels)
        validation_time = time.time() - validation_start
        
        self.log(f"éªŒè¯å®Œæˆï¼æœ‰æ•ˆé¢‘é“: {len(valid_channels)}/{len(quality_channels)}")
        
        # åˆ†ç±»é¢‘é“
        categorized_channels = {'cctv': [], 'satellite': [], 'local': [], 'international': [], 'other': []}
        for channel in valid_channels:
            category = self.categorize_channel(channel['name'])
            categorized_channels[category].append(channel)
        
        # ç”ŸæˆéªŒè¯åçš„M3Uæ–‡ä»¶
        validated_content = self.generate_m3u_content(valid_channels, "å·²éªŒè¯ç›´æ’­æº")
        with open('outputs/full_validated.m3u', 'w', encoding='utf-8') as f:
            f.write(validated_content)
        
        # ç”Ÿæˆåˆ†ç±»æ–‡ä»¶
        for category, channels in categorized_channels.items():
            if channels:
                category_content = self.generate_m3u_content(channels, f"{category}é¢‘é“")
                with open(f'outputs/{category}.m3u', 'w', encoding='utf-8') as f:
                    f.write(category_content)
        
        # ä¿å­˜æ—¥å¿—
        with open('logs/latest_update.log', 'w', encoding='utf-8') as f:
            f.write('\n'.join(self.log_messages))
        
        # ç»Ÿè®¡ä¿¡æ¯
        end_time = time.time()
        duration = end_time - start_time
        validity_ratio = len(valid_channels) / len(quality_channels) if len(quality_channels) > 0 else 0
        
        stats = {
            'update_time': datetime.now().isoformat(),
            'duration_seconds': round(duration, 2),
            'validation_seconds': round(validation_time, 2),
            'sources_attempted': len(self.sources_config['sources']) + len(self.sources_config['backup_sources']),
            'sources_successful': successful_sources,
            'total_channels': len(unique_channels_list),
            'quality_channels': len(quality_channels),
            'valid_channels': len(valid_channels),
            'validity_ratio': validity_ratio,
            'categories': {k: len(v) for k, v in categorized_channels.items()}
        }
        
        with open('outputs/stats.json', 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        # æ›´æ–°READMEæ–‡ä»¶
        self.update_readme(stats)
        
        self.log(f"=== æ›´æ–°å®Œæˆï¼è€—æ—¶: {duration:.2f}ç§’ ===")
        self.log(f"ç»Ÿè®¡ä¿¡æ¯: {stats}")

if __name__ == "__main__":
    updater = IPTVUpdater()
    updater.run()    
    def fetch_source(self, url, timeout=15):
        """è·å–å•ä¸ªæº"""
        try:
            self.log(f"æ­£åœ¨è·å–: {url}")
            response = self.session.get(url, timeout=timeout)
            response.encoding = 'utf-8'
            if response.status_code == 200:
                return response.text
            else:
                self.log(f"è·å–å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return None
        except Exception as e:
            self.log(f"è·å–å¼‚å¸¸: {e}")
            return None
    
    def parse_m3u(self, content, source_url):
        """è§£æM3Uå†…å®¹"""
        channels = []
        current_channel = {}
        lines = content.splitlines()
        
        for i, line in enumerate(lines):
            line = line.strip()
            if not line:
                continue
                
            if line.startswith('#EXTINF'):
                # è§£æé¢‘é“ä¿¡æ¯
                current_channel = {'raw_extinf': line}
                # æå–é¢‘é“åç§°
                name_match = re.search(r',(?P<name>.*)$', line)
                if name_match:
                    current_channel['name'] = name_match.group('name').strip()
                else:
                    current_channel['name'] = f"Unknown_{i}"
                    
            elif line.startswith('http://') or line.startswith('https://') or line.startswith('rtsp://') or line.startswith('rtmp://'):
                if current_channel:
                    current_channel['url'] = line
                    current_channel['source'] = source_url
                    channels.append(current_channel)
                    current_channel = {}
        
        self.log(f"ä»è¯¥æºè§£æå‡º {len(channels)} ä¸ªé¢‘é“")
        return channels
    
    def is_url_accessible(self, url, timeout=8):
        """æ£€æŸ¥URLæ˜¯å¦å¯è®¿é—®"""
        try:
            # åªå‘é€HEADè¯·æ±‚æ£€æŸ¥ï¼ŒèŠ‚çœæ—¶é—´å’Œå¸¦å®½
            response = requests.head(url, timeout=timeout, allow_redirects=True)
            return response.status_code in [200, 302, 301]
        except:
            try:
                # å¦‚æœHEADå¤±è´¥ï¼Œå°è¯•GETä½†åªè¯»å–å¤´ä¿¡æ¯
                response = requests.get(url, timeout=timeout, stream=True)
                return response.status_code == 200
            except:
                return False
    
    def categorize_channel(self, channel_name):
        """åˆ†ç±»é¢‘é“"""
        name_lower = channel_name.lower()
        
        # å¤®è§†åˆ†ç±»
        cctv_keywords = ['cctv', 'å¤®è§†', 'ä¸­å¤®']
        if any(keyword in name_lower for keyword in cctv_keywords):
            return 'cctv'
        
        # å«è§†åˆ†ç±»
        satellite_keywords = ['å«è§†', 'tvb', 'å‡¤å‡°', 'æ˜Ÿç©º', 'æ¹–å—', 'æµ™æ±Ÿ', 'æ±Ÿè‹', 'ä¸œæ–¹', 'åŒ—äº¬']
        if any(keyword in name_lower for keyword in satellite_keywords):
            return 'satellite'
        
        # åœ°æ–¹å°åˆ†ç±»
        local_keywords = ['éƒ½å¸‚', 'æ–°é—»', 'æ°‘ç”Ÿ', 'å…¬å…±', 'æ•™è‚²', 'å°‘å„¿', 'ä½“è‚²', 'å½±è§†', 'ç»¼è‰º']
        if any(keyword in name_lower for keyword in local_keywords):
            return 'local'
        
        # å›½é™…é¢‘é“
        international_keywords = ['bbc', 'cnn', 'nhk', 'fox', 'hbo', 'disney', 'discovery', 'å›½å®¶åœ°ç†']
        if any(keyword in name_lower for keyword in international_keywords):
            return 'international'
        
        return 'other'
    
    def generate_m3u_content(self, channels, category=None):
        """ç”ŸæˆM3Uå†…å®¹"""
        if category:
            header = f"""#EXTM3U
#EXTENC: UTF-8
# Generated: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')}
# Updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
# Category: {category.upper()}
# Total Channels: {len(channels)}
# For personal testing and research purposes only.

"""
        else:
            header = f"""#EXTM3U
#EXTENC: UTF-8
# Generated: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')}
# Updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
# Total Channels: {len(channels)}
# For personal testing and research purposes only.

"""
        
        content = header
        for channel in channels:
            content += f"{channel['raw_extinf']}\n{channel['url']}\n"
        
        return content
    
    def update_readme(self, stats):
        """æ›´æ–°READMEæ–‡ä»¶ï¼Œæ·»åŠ ç›´æ’­æºåœ°å€"""
        try:
            # è¯»å–ç°æœ‰çš„READMEå†…å®¹
            if not os.path.exists('README.md'):
                # å¦‚æœREADMEä¸å­˜åœ¨ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„
                with open('README.md', 'w', encoding='utf-8') as f:
                    f.write('# DailyIPTV ğŸ“º\n\n')
                readme_content = '# DailyIPTV ğŸ“º\n\n'
            else:
                with open('README.md', 'r', encoding='utf-8') as f:
                    readme_content = f.read()
            
            # ç”Ÿæˆç›´æ’­æºåœ°å€éƒ¨åˆ†
            base_url = f"https://raw.githubusercontent.com/{self.repository_owner}/{self.repository_name}/main/outputs"
            update_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            
            live_sources_section = f"""
## ğŸ“¡ ç›´æ’­æºåœ°å€

ä»¥ä¸‹æ˜¯æœ€æ–°çš„ç›´æ’­æºåœ°å€ï¼ˆæœ€åæ›´æ–°: {update_time}ï¼‰ï¼š

### ğŸŒ å®Œæ•´åˆ—è¡¨
- **M3Uæ ¼å¼**: [{base_url}/full.m3u]({base_url}/full.m3u)
- é¢‘é“æ€»æ•°: {stats['total_channels']} ä¸ª
- ä¼°è®¡æœ‰æ•ˆé¢‘é“: {stats['estimated_valid_channels']} ä¸ª
- æœ‰æ•ˆæ€§æ¯”ä¾‹: {stats['validity_ratio']:.2%}

### ğŸ“º åˆ†ç±»åˆ—è¡¨
- **å¤®è§†é¢‘é“**: [{base_url}/cctv.m3u]({base_url}/cctv.m3u) ({stats['categories']['cctv']} ä¸ªé¢‘é“)
- **å«è§†é¢‘é“**: [{base_url}/satellite.m3u]({base_url}/satellite.m3u) ({stats['categories']['satellite']} ä¸ªé¢‘é“)
- **åœ°æ–¹å°**: [{base_url}/local.m3u]({base_url}/local.m3u) ({stats['categories']['local']} ä¸ªé¢‘é“)
- **å›½é™…é¢‘é“**: [{base_url}/international.m3u]({base_url}/international.m3u) ({stats['categories']['international']} ä¸ªé¢‘é“)
- **å…¶ä»–é¢‘é“**: [{base_url}/other.m3u]({base_url}/other.m3u) ({stats['categories']['other']} ä¸ªé¢‘é“)

### ğŸ“Š ç»Ÿè®¡ä¿¡æ¯
- **æ›´æ–°è€—æ—¶**: {stats['duration_seconds']} ç§’
- **æºå°è¯•æ•°**: {stats['sources_attempted']} ä¸ª
- **æˆåŠŸæºæ•°**: {stats['sources_successful']} ä¸ª
- **æ›´æ–°æ—¶é—´**: {stats['update_time']}

### ğŸš€ å¿«é€Ÿä½¿ç”¨
åœ¨æ”¯æŒM3Uçš„æ’­æ”¾å™¨ï¼ˆVLCã€PotPlayerã€Kodiç­‰ï¼‰ä¸­ï¼š
1. æ‰“å¼€"æ‰“å¼€ç½‘ç»œæµ"
2. ç²˜è´´ä¸Šè¿°ä»»æ„é“¾æ¥
3. äº«å—ç›´æ’­ï¼

---

"""
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰ç›´æ’­æºåœ°å€éƒ¨åˆ†
            if '## ğŸ“¡ ç›´æ’­æºåœ°å€' in readme_content:
                # æ›¿æ¢ç°æœ‰çš„ç›´æ’­æºéƒ¨åˆ†
                pattern = r'## ğŸ“¡ ç›´æ’­æºåœ°å€.*?---'
                updated_readme = re.sub(pattern, live_sources_section.strip(), readme_content, flags=re.DOTALL)
            else:
                # åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ ç›´æ’­æºéƒ¨åˆ†
                if readme_content.startswith('# DailyIPTV ğŸ“º'):
                    updated_readme = readme_content.replace('# DailyIPTV ğŸ“º', f'# DailyIPTV ğŸ“º{live_sources_section}')
                else:
                    updated_readme = f'# DailyIPTV ğŸ“º{live_sources_section}\n\n{readme_content}'
            
            # å†™å…¥æ›´æ–°åçš„README
            with open('README.md', 'w', encoding='utf-8') as f:
                f.write(updated_readme)
            
            self.log("READMEæ–‡ä»¶æ›´æ–°æˆåŠŸï¼")
            return True
            
        except Exception as e:
            self.log(f"æ›´æ–°READMEæ–‡ä»¶å¤±è´¥: {e}")
            import traceback
            self.log(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return False

    def generate_complete_playlist(self, all_channels):
        """ç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„æ’­æ”¾åˆ—è¡¨ï¼ŒåŒ…å«æ‰€æœ‰é¢‘é“"""
        self.log("ç”Ÿæˆå®Œæ•´æ’­æ”¾åˆ—è¡¨...")
        
        # æŒ‰åˆ†ç±»ç»„ç»‡é¢‘é“
        categorized = {
            'cctv': [], 'satellite': [], 'local': [], 
            'international': [], 'other': []
        }
        
        for channel in all_channels:
            category = self.categorize_channel(channel['name'])
            categorized[category].append(channel)
        
        # ç”Ÿæˆå®Œæ•´M3Uå†…å®¹
        complete_content = self.generate_m3u_content(all_channels)
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs('outputs', exist_ok=True)
        
        # å†™å…¥å®Œæ•´æ–‡ä»¶
        with open('outputs/full.m3u', 'w', encoding='utf-8') as f:
            f.write(complete_content)
        
        # å†™å…¥åˆ†ç±»æ–‡ä»¶
        for category, channels in categorized.items():
            if channels:
                category_content = self.generate_m3u_content(channels, category)
                with open(f'outputs/{category}.m3u', 'w', encoding='utf-8') as f:
                    f.write(category_content)
        
        return categorized

    def run(self):
        """ä¸»è¿è¡Œå‡½æ•°"""
        start_time = time.time()
        self.log("=== å¼€å§‹æ›´æ–°IPTVç›´æ’­æº ===")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs('outputs', exist_ok=True)
        os.makedirs('logs', exist_ok=True)
        
        # åŠ è½½é…ç½®
        self.sources_config = self.load_sources()
        
        # è·å–æ‰€æœ‰æº
        all_channels = []
        successful_sources = 0
        
        # å¤„ç†ä¸»è¦æº
        sources_to_try = self.sources_config.get('sources', [])
        backup_sources = self.sources_config.get('backup_sources', [])
        
        self.log(f"å°è¯• {len(sources_to_try)} ä¸ªä¸»è¦æºå’Œ {len(backup_sources)} ä¸ªå¤‡ç”¨æº")
        
        for source_url in sources_to_try:
            content = self.fetch_source(source_url)
            if content:
                channels = self.parse_m3u(content, source_url)
                all_channels.extend(channels)
                successful_sources += 1
                time.sleep(1)  # ç¤¼è²Œå»¶è¿Ÿ
        
        # å¦‚æœä¸»è¦æºå¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æº
        if successful_sources == 0 and backup_sources:
            self.log("ä¸»è¦æºå…¨éƒ¨å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æº...")
            for backup_url in backup_sources:
                content = self.fetch_source(backup_url)
                if content:
                    channels = self.parse_m3u(content, backup_url)
                    all_channels.extend(channels)
                    successful_sources += 1
                    time.sleep(1)
        
        if not all_channels:
            self.log("é”™è¯¯ï¼šæ— æ³•ä»ä»»ä½•æºè·å–é¢‘é“æ•°æ®")
            return
        
        # å»é‡ï¼šåŸºäºURLå»é‡
        unique_channels = {}
        for channel in all_channels:
            url = channel['url']
            if url not in unique_channels:
                unique_channels[url] = channel
        
        unique_channels_list = list(unique_channels.values())
        self.log(f"å»é‡åé¢‘é“æ•°é‡: {len(unique_channels_list)}")
        
        # éªŒè¯é¢‘é“æœ‰æ•ˆæ€§ï¼ˆæŠ½æ ·éªŒè¯ï¼Œé¿å…è€—æ—¶è¿‡é•¿ï¼‰
        self.log("å¼€å§‹éªŒè¯é¢‘é“æœ‰æ•ˆæ€§ï¼ˆæŠ½æ ·æ£€æŸ¥ï¼‰...")
        valid_count = 0
        sample_size = min(100, len(unique_channels_list))
        
        for i, channel in enumerate(unique_channels_list[:sample_size]):
            if self.is_url_accessible(channel['url']):
                valid_count += 1
            if (i + 1) % 10 == 0 or (i + 1) == sample_size:
                self.log(f"å·²éªŒè¯ {i+1}/{sample_size} ä¸ªé¢‘é“")
        
        # è®¡ç®—æœ‰æ•ˆæ€§æ¯”ä¾‹
        validity_ratio = valid_count / sample_size if sample_size > 0 else 0
        estimated_valid_count = int(len(unique_channels_list) * validity_ratio)
        self.log(f"æœ‰æ•ˆæ€§æŠ½æ ·æ¯”ä¾‹: {validity_ratio:.2%}, ä¼°è®¡æœ‰æ•ˆé¢‘é“: {estimated_valid_count}")
        
        # ç”Ÿæˆå®Œæ•´çš„æ’­æ”¾åˆ—è¡¨
        categorized_channels = self.generate_complete_playlist(unique_channels_list)
        
        # ç»Ÿè®¡ä¿¡æ¯
        end_time = time.time()
        duration = end_time - start_time
        
        stats = {
            'update_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'duration_seconds': round(duration, 2),
            'sources_attempted': len(sources_to_try) + len(backup_sources),
            'sources_successful': successful_sources,
            'total_channels': len(unique_channels_list),
            'estimated_valid_channels': estimated_valid_count,
            'validity_ratio': validity_ratio,
            'categories': {k: len(v) for k, v in categorized_channels.items()}
        }
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        with open('outputs/stats.json', 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜æ—¥å¿—
        with open('logs/latest_update.log', 'w', encoding='utf-8') as f:
            f.write('\n'.join(self.log_messages))
        
        # æ›´æ–°READMEæ–‡ä»¶
        self.log("æ­£åœ¨æ›´æ–°READMEæ–‡ä»¶...")
        update_success = self.update_readme(stats)
        if update_success:
            self.log("READMEæ–‡ä»¶æ›´æ–°æˆåŠŸï¼")
        else:
            self.log("READMEæ–‡ä»¶æ›´æ–°å¤±è´¥ï¼")
        
        self.log(f"=== æ›´æ–°å®Œæˆï¼è€—æ—¶: {duration:.2f}ç§’ ===")
        self.log(f"ç»Ÿè®¡ä¿¡æ¯: {json.dumps(stats, ensure_ascii=False, indent=2)}")
        
        # è¾“å‡ºæ–‡ä»¶ä¿¡æ¯
        self.log("ç”Ÿæˆçš„æ–‡ä»¶:")
        for file in os.listdir('outputs'):
            if file.endswith('.m3u'):
                file_path = os.path.join('outputs', file)
                file_size = os.path.getsize(file_path)
                self.log(f"  - {file}: {file_size} bytes")

if __name__ == "__main__":
    updater = IPTVUpdater()
    updater.run()
